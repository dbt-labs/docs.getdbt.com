---
title: "Quickstart for dbt Mesh"
id: "mesh-qs"
level: 'Intermediate'
icon: 'guides'
tags: ['dbt Cloud','Quickstart']
hide_table_of_contents: true
---

<div style={{maxWidth: '900px'}}>

## Introduction

In this guide youâ€™ll walk through how to set up a multi-project design using foundational concepts of [dbt Mesh](https://www.getdbt.com/blog/what-is-data-mesh-the-definition-and-importance-of-data-mesh). In this example, youâ€™ll:
- Set up a foundational (i.e. hub) project called â€œJaffle | Data Analyticsâ€
- Set up a downstream (i.e. spoke) project called â€œJaffle | Financeâ€
- Add model access, versions, and contracts 
- Set up a dbt Cloud Job that is triggered on completion of an upstream job

This framework enables organizations to scale â€“ across both teams and data assets â€“ in a way that enforces governance best practices while distributing large projects into manageable data domainsâ€”for faster data development. 

<!-- :::tip Videos for you
You can check out [dbt Fundamentals](https://courses.getdbt.com/courses/fundamentals) for free if you're interested in course learning with videos.

You can also watch the [YouTube video on dbt and Snowflake](https://www.youtube.com/watch?v=kbCkwhySV_I&list=PL0QYlrC86xQm7CoOH6RS7hcgLnd3OQioG).
::: -->
 
### Prerequisitesâ€‹

<!-- :::warning

This guide assumes you have a [dbt Cloud Enterprise Account](https://www.getdbt.com/get-started/enterprise-contact-pricing) (required for multiple projects)

::: -->

- You have access to a cloud data warehouse, permissions to load the sample data tables, and dbt Cloud permissions to create new projects.
- Load the Jaffle Shop sample data (customers, orders, and payments tables) into your data platform. You can find the steps to do so for each platform, respectively, here:
    - [Snowflake](https://docs.getdbt.com/guides/snowflake?step=3)
    - [Databricks](https://docs.getdbt.com/guides/databricks?step=3)
    - [Redshift](https://docs.getdbt.com/guides/redshift?step=3)
    - [BigQuery](https://docs.getdbt.com/guides/bigquery?step=3)
    - [Fabric](https://docs.getdbt.com/guides/microsoft-fabric?step=2)
    - [Starburst Galaxy](https://docs.getdbt.com/guides/starburst-galaxy?step=2)
- Set dbt [version](https://docs.getdbt.com/docs/dbt-versions/core) to 1.6 or later in your Development and Deployment Environments
- This guide assumes you have experience with or fundamental knowledge of dbt. Take the [dbt Fundamentals](https://courses.getdbt.com/courses/fundamentals) course first if you are brand new to dbt.

### Related content:
- [Data mesh concepts: What it is and how to get started](https://www.getdbt.com/blog/data-mesh-concepts-what-it-is-and-how-to-get-started)
- [Deciding how to structure your dbt Mesh](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-2-structures)
- [Implementation guide](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-3-implementation)
- [dbt Mesh FAQs](https://docs.getdbt.com/best-practices/how-we-mesh/mesh-4-faqs)

## Create and configure two new dbt Cloud Projects

Weâ€™ll create two new, empty projects in dbt Cloud to use in this tutorial. Here, the always-enterprising Jaffle Labs is setting up a project for their Data Analytics and Finance team, respectively:

<!-- <Lightbox src="/img/guides/dbt-mesh/project_names.png" title="Two new dbt Cloud Projects" /> -->

Once each project is [configured](https://docs.getdbt.com/docs/cloud/about-cloud-setup) with a) a data platform connection, b) new git repo, and c) 1+ Environments weâ€™re ready to begin. 

## Set up a foundational (i.e. hub) project

Before a downstream team can leverage assets from this foundational project, weâ€™ll need to first:
- Create and define at least one model as â€œpublicâ€
- Run a deployment job successfully 
    - Note: toggle on â€œGenerate docs on runâ€ for this job to update dbt Explorer. Once run, you can tie Explorer to a deployment job on the Project Settings page (details [here](https://docs.getdbt.com/docs/deploy/artifacts)).

Letâ€™s walk through setting this up in our â€œJaffle | Data Analyticsâ€ project. First, head to the â€œDevelopâ€ page to verify our setup. Click â€œInitializing dbt projectâ€ if youâ€™ve started with an empty repo:

<!-- <Lightbox src="/img/guides/dbt-mesh/initialize_repo.png" title="Initialize repo" /> -->

Delete the â€œmodels/exampleâ€ folder and remove lines 39-42 from the dbt_project.yml file. While there, letâ€™s also rename the project (line 5) from â€œmy_new_projectâ€ to â€œanalyticsâ€.

Create two new folders: â€œmodels/stagingâ€ and â€œmodels/coreâ€.

Weâ€™ll set up our staging layer as follows:
1. Create a new YML file `models/staging/sources.yml`.
2. Declare the sources by copying the following into the file and clicking **Save**.

    <File name='models/staging/sources.yml'>

    ```yml
    version: 2

    sources:
        - name: jaffle_shop
          description: This is a replica of the Postgres database used by our app
          database: raw
          schema: jaffle_shop
          tables:
              - name: customers
                description: One record per customer.
              - name: orders
                description: One record per order. Includes cancelled and deleted orders.
    ```

    </File>

3. Create a `models/staging/stg_customers.sql` file to select from the `customers` table in the `jaffle_shop` source.

    <File name='models/staging/stg_customers.sql'>

    ```sql
    select
        id as customer_id,
        first_name,
        last_name

    from {{ source('jaffle_shop', 'customers') }}
    ```

    </File>

4. Create a `models/staging/stg_orders.sql` file to select from the `orders` table in the `jaffle_shop` source.

    <File name='models/staging/stg_orders.sql'>

    ```sql
    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from {{ source('jaffle_shop', 'orders') }}
    ```

    </File>

5. Create a `models/core/fct_orders.sql` file to build a fact table with customer and order details

    <File name='models/core/fct_orders.sql'>

    ```sql
    with customers as (

    select * from {{ ref('stg_customers') }}

    ),

    orders as (

        select * from {{ ref('stg_orders') }}

    ),

    customer_orders as (

        select
            customer_id,
            min(order_date) as first_order_date,

        from orders

        group by 1

    ),

    final as (

        select 
            orders.order_id,
            orders.order_date,
            orders.status,
            
            customers.customer_id,
            customers.first_name,
            customers.last_name,
            
            customer_orders.first_order_date,
            -- Note that we've used a macro for this so that the appropriate DATEDIFF syntax is used for each respective data platform
            {{ dbt.datediff("first_order_date", "order_date", "day") }} as days_as_customer_at_purchase

        from orders 
        left join customers using (customer_id)
        left join customer_orders using (customer_id)

    )

    select * from final

    ```

    </File>

6. Execute `dbt build`. 

## Define a public model & run first job

Alright, we have our basic building blocks arranged. How does dbt Mesh come in?

We know our Finance team will need to build on fct_orders as they analyze payment trends. That said, many of the other models in our project â€“ namely the staging layer where we cleansed and joined the data, in this example â€“ arenâ€™t needed by downstream teams.

To make fct_orders available, weâ€™ll just need to add an â€œaccess: publicâ€ clause to the relevant yml file. Letâ€™s add the following file into the project and save it:

<File name='models/core/core.yml'>

```yml

version: 2

models:
  - name: fct_orders
    access: public
    description: "Customer and order details"
    config:
    contract:
        enforced: true
    columns:
      - name: order_id
        data_type: number
        description: ""

      - name: order_date
        data_type: date
        description: ""

      - name: status
        data_type: varchar
        description: "Indicates the status of the order"

      - name: customer_id
        data_type: number
        description: ""

      - name: first_name
        data_type: varchar
        description: ""

      - name: last_name
        data_type: varchar
        description: ""

      - name: first_order_date
        data_type: date
        description: ""

      - name: days_as_customer_at_purchase
        data_type: number
        escription: "Days between this purchase and customer's first purchase"

```

</File>

Note: if unspecified, model access defaults to â€œprotectedâ€ (i.e. referenceable in the same project only). You can learn more about access types and model groups [here](https://docs.getdbt.com/docs/collaborate/govern/model-access#access-modifiers).

Now in the â€œLineageâ€ tab we should see the model noted as â€œPublicâ€:

<!-- <Lightbox src="/img/guides/dbt-mesh/da_lineage.png" title="Jaffle | Data Analytics lineage" /> -->

Letâ€™s â€œCommit & Syncâ€ our changes, then merge them to the main/prod branch.

Last but not least, letâ€™s run our first deployment dbt Cloud Job. Go to the â€œDeployâ€ -> â€œ Jobsâ€ page. Click â€œCreate jobâ€ -> â€œDeploy jobâ€.

Check the â€œGenerate docs on runâ€ box so that we can later represent the state of this project in the Explore page. Then, click â€œRun Nowâ€.
<!-- <Lightbox src="/img/guides/dbt-mesh/generate_docs_on_run.png" title="Jaffle | Generate docs on run" /> -->
<!-- <Lightbox src="/img/guides/dbt-mesh/jon_run_now.png" title="Jaffle | Trigger job" /> -->

After the run is complete, go to the Project Setting page and link Documentation to the Job:
<!-- <Lightbox src="/img/guides/dbt-mesh/set_project_artifacts.png" title="Configure project artifacts" /> -->

Click â€œExploreâ€ from the upper menu bar. If all has gone to plan, we should now see our lineage, tests, and documentation coming through successfully!

## Reference a public model in your downstream (i.e. spoke) project

Now, letâ€™s switch over to Jaffle | Finance and head to the â€œDevelopâ€ page. A couple quick tasks to set up our project:
1. Click â€œInitialize dbt projectâ€ (in the upper left) if youâ€™ve also started with an new git repo
2. Delete the â€œmodels/exampleâ€ folder and remove lines 39-42 from the dbt_project.yml file 
3. Create a â€œdependencies.ymlâ€ file. Add the upstream â€œanalyticsâ€ project and the dbt_utils package.
<!-- <Lightbox src="/img/guides/dbt-mesh/finance_create_file.png" title="Jaffle | Create file" /> -->

<File name='dependencies.yml'>

```yml

packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1

projects:
  - name: analytics

```

</File>

Add the following files:

4. models/staging/sources.yml

    <File name='models/staging/sources.yml'>

    ```yml
    version: 2

    sources:
    - name: stripe
        database: raw
        schema: stripe 
        tables:
        - name: payment
    ```

    </File>

5. models/staging/stg_payments.sql

    <File name='models/staging/stg_payments.sql'>

    ```sql

    with payments as (
        select * from {{ source('stripe', 'payment') }}
    ),

    final as (
        select 
            id as payment_id,
            "orderID" as order_id,
            "paymentMethod" as payment_method,
            amount,
            created as payment_date 
        from payments
    )

    select * from final

    ```

    </File> 

Alright, now weâ€™re ready to add the model to answer the question the exec team would like to understand: how does payment type change through a customer journey? This will be important to understand if coupon gift card amounts decrease as a customer repeats purchases (as hoped by our marketing team) or persists. 

6. We can use the following logic to ascertain this:

    <File name='models/core/agg_customer_payment_journey.sql'>

    ```sql

    with stg_payments as (
        select * from {{ ref('stg_payments') }}
    ),

    fct_orders as (
        select * from {{ ref('analytics', 'fct_orders') }}
    ),

    final as (
        select 
            days_as_customer_at_purchase,
            -- we use the pivot macro in the dbt_utils package to create columns that total payments for each method
            {{ dbt_utils.pivot(
                'payment_method',
                dbt_utils.get_column_values(ref('stg_payments'), 'payment_method'),
                agg='sum',
                then_value='amount',
                prefix='total_',
                suffix='_amount'
            ) }}, 
            sum(amount) as total_amount
        from fct_orders
        left join stg_payments using (order_id)
        group by 1
    )

    select * from final

    ```

    </File> 


Notice the cross-project ref at work! As we add the familiar ref, the auto-complete helper in the IDE also detects the public model as available.
<!-- <Lightbox src="/img/guides/dbt-mesh/cross_proj_ref_autocomplete.png" title="Cross-project ref autocomplete" /> -->

This resolves to the appropriate database, schema, and table/view as defined by the upstream project:
<!-- <Lightbox src="/img/guides/dbt-mesh/cross_proj_ref_compile.png" title="Cross-project ref compile" /> -->

We now see this represented in the live â€œlineageâ€ tab as well:
<!-- <Lightbox src="/img/guides/dbt-mesh/cross_proj_ref_lineage.png" title="Cross-project ref lineage" /> -->

## Enrich this handoff with model versions & contracts
How can we build in additional resilience and guardrails to this type of multi-project relationship? Pulling from software engineering best practices, we can now:
1. Define model [contracts](https://docs.getdbt.com/docs/collaborate/govern/model-contracts) in dbt to define a set of upfront "guarantees" that define the shape of your model. While building your model, dbt will verify that your model's transformation will produce a dataset matching up with its contract, or it will fail to build.
2. Define model [versions](https://docs.getdbt.com/docs/collaborate/govern/model-versions) to offer a clear migration path in cases where a breaking change is required

Letâ€™s switch back to our example: weâ€™re on the Data Analytics team and want to set guarantees on fct_orders so that downstream teams â€“ like the Finance team â€“ can be certain of its reliability.

Add a data contract to fct_orders. Go to â€œmodels/core/core.ymlâ€ and add the following underneath the model name:
```yml
models:
  - name: fct_orders
    access: public
    description: "Customer and order details"
    config:
      contract:
        enforced: true

```
So, what would happen if this contract were violated? Letâ€™s see. Open up â€œmodels/core/fct_orders.sqlâ€, comment out the â€œorders.statusâ€ column, and click [Build](https://docs.getdbt.com/docs/cloud/dbt-cloud-ide/develop-in-the-cloud#build-compile-and-run-projects) to try building the model.

In the command bar history, we can see that the contract failed because the status field was removed:
<!-- <Lightbox src="/img/guides/dbt-mesh/break_contract.png" title="Broken contract" /> -->

Now, letâ€™s walk through how model versions can be used by the Data Analytics team as they upgrade the fct_orders model while offering backward compatibility and a migration notice to the downstream Finance team.

Rename â€œmodels/core/fct_orders.sqlâ€ to â€œmodels/core/fct_orders_v1.sqlâ€ and create a new file named â€œmodels/core/fct_orders_v2.sqlâ€:

In â€œmodels/core/fct_orders_v2.sqlâ€, comment out â€œorders.statusâ€ and instead add a new field, is_return, that yields true if status = â€˜returnedâ€™ and false otherwise.

Then, add the following to your â€œmodels/core/core.ymlâ€ file:
- the â€œis_returnâ€ column
- the two model versions
- a latest_version to indicate which model is the latest (and should be used by default, unless specified otherwise)
- a deprecation_date to version 1 as well to indicate 

It should now read as follows:
<File name='models/core/core.yml'>

```yml

version: 2

models:
  - name: fct_orders
    access: public
    description: "Customer and order details"
    latest_version: 2
    config:
      contract:
        enforced: true
    columns:
      - name: order_id
        data_type: number
        description: ""

      - name: order_date
        data_type: date
        description: ""

      - name: status
        data_type: varchar
        description: "Indicates the status of the order"

      - name: is_return
        data_type: boolean
        description: "Indicates if an order was returned"

      - name: customer_id
        data_type: number
        description: ""

      - name: first_name
        data_type: varchar
        description: ""

      - name: last_name
        data_type: varchar
        description: ""

      - name: first_order_date
        data_type: date
        description: ""

      - name: days_as_customer_at_purchase
        data_type: number
        description: "Days between this purchase and customer's first purchase"

    # Declare the versions, and highlight the diffs
    versions:
    
      - v: 1
        deprecation_date: 2024-06-30 00:00:00.00+00:00
        columns:
          # This means: use the 'columns' list from above, but exclude is_return
          - include: all
            exclude: [is_return]
        
      - v: 2
        columns:
          # This means: use the 'columns' list from above, but exclude status
          - include: all
            exclude: [status]


```

</File>

Letâ€™s verify how dbt compiles the ref statement now based on the version that we scope. Open a new file, add the following select statements, and click â€œCompileâ€.

```sql
select * from {{ ref('fct_orders', v=1) }}
select * from {{ ref('fct_orders', v=2) }}
select * from {{ ref('fct_orders') }}
```

## Add dbt Cloud Job in the downstream project
Before proceeding, commit & merge your changes in both the â€œJaffle | Data Analyticsâ€ and â€œJaffle | Financeâ€ projects. 

A member of the Finance team would like to schedule a dbt Cloud Job their customer payment journey analysis immediately after the Data Analytics team refreshes their pipelines.

In the â€œJaffle | Financeâ€ project, go to the Jobs page (â€œDeployâ€ -> â€œJobsâ€) and click â€œCreate jobâ€ -> â€œDeploy jobâ€. Add a name for the job, then scroll to the bottom and configure the job to â€œRun when another job finishesâ€. Select the upstream job, like so:
<!-- <Lightbox src="/img/guides/dbt-mesh/trigger_on_completion.png" title="Trigger job on completion" /> -->

Click â€œSaveâ€. Letâ€™s verify this works as we expect. Go to the â€œJaffle | Data Analyticsâ€ Jobs page, select the â€œDaily Jobâ€, and click â€œRun Nowâ€. Once this job completes successfully, go back the â€œJaffle | Financeâ€ Jobs page. Boom! The Finance teamâ€™s job was triggered automatically.

This makes it dead simple to stay in sync with the upstream tables and removes the need for more sophisticated orchestration skills (e.g. coordinating jobs across projects via an external orchestrator).

## View deprecation warning

How would a finance team member know how long they have to migrate over from fct_orders_v1 to fct_orders_v2?

Letâ€™s check. Head back to the â€œDevelopâ€ page in the â€œJaffle | Financeâ€ project. Edit the cross project ref to use v=1 in â€œmodels/marts/agg_customer_payment_journey.sqlâ€:

<File name='models/core/agg_customer_payment_journey.sql'>

```sql

with stg_payments as (
    select * from {{ ref('stg_payments') }}
),

fct_orders as (
    select * from {{ ref('analytics', 'fct_orders', v=1) }}
),

final as (
    select 
        days_as_customer_at_purchase,
        -- we use the pivot macro in the dbt_utils package to create columns that total payments for each method
        {{ dbt_utils.pivot(
            'payment_method',
            dbt_utils.get_column_values(ref('stg_payments'), 'payment_method'),
            agg='sum',
            then_value='amount',
            prefix='total_',
            suffix='_amount'
        ) }}, 
        sum(amount) as total_amount
    from fct_orders
    left join stg_payments using (order_id)
    group by 1
)

select * from final

```

</File>

Then, commit & merge the change. Go to the â€œDeploy -> Jobsâ€ page and click â€œRun Nowâ€ on the Finance Job. Weâ€™ll see that the the agg_customer_payment_journey model builds but surfaces a warning with the deprecation date.

<!-- <Lightbox src="/img/guides/dbt-mesh/deprecation_date_warning.png" title="Deprecation date warning" /> -->

## View lineage with dbt Explorer

In dbt Cloud, navigate to the **Explore** page for each of your projects &mdash; you should now view the [lineage seamlessly across projects](/docs/collaborate/explore-multiple-projects)!

Congratulations ğŸ‰! You're ready to bring the benefits of dbt Mesh to your organization.

</div>
