---
title: "Available materializations"
id: materializations-guide-2-available-materializations
slug: 2-available-materializations
description: Read this guide to understand the different types of materializations you can create in dbt.
displayText: Materializations best practices
hoverSnippet: Read this guide to understand the different types of materializations you can create in dbt.
---

Views and tables and incremental models, oh my! In this section weâ€™ll start getting our hands dirty digging into the three basic materializations that ship with dbt. They are considerably less scary and more helpful than lions, tigers, or bears â€” although perhaps not as cute (can data be cute? We at dbt Labs think so). Weâ€™re going to define, implement, and explore:

- ğŸ” **views**
- âš’ï¸Â **tables**
- ğŸ“šÂ **incremental model**

:::info
ğŸ‘» There is a fourth default materialization available in dbt called **ephemeral materialization**. It is less broadly applicable than the other three, and better deployed for specific use cases that require weighing some tradeoffs. We chose to leave it out of this guide and focus on the three materializations that will power 99% of your modeling needs.
:::

**Views and Tables are the two basic categories** of object that we can create across warehouses. They exist natively as types of objects in the warehouse, as you can see from this screenshot of Snowflake (depending on your warehouse the interface will look a little different). **Incremental models** and other materializations types are a little bit different. They tell dbt to **construct tables in a special way**.

![Tables and views in the browser on Snowflake.](/img/guides/best-practices/materializations/tables-and-views.png)

### Views

- âœ…Â **The default materialization in dbt**. A starting project has no configurations defined for materializations, which means _everything_ is by default built as a view.
- ğŸ‘©â€ğŸ’»Â **Store _only the SQL logic_ of the transformation in the warehouse, _not the data_**. As such, they make a great default. They build almost instantly and cost almost nothing to build.
- â±ï¸Â Always reflect the **most up-to-date** version of the input data, as theyâ€™re run freshly every time theyâ€™re queried.
- ğŸ‘Â **Have to be processed every time theyâ€™re queried, so slower to return results than a table of the same data.** That also means they can cost more over time, especially if they contain intensive transformations and are queried often.

### Tables

- ğŸ—ï¸Â **Tables store the data itself** as opposed to views which store the query logic. This means we can pack all of the transformation compute into a single run. A view is storing a _query_ in the warehouse. Even to preview that data we have to query it. A table is storing the literal rows and columns on disk.
- ğŸï¸Â Querying lets us **access that transformed data directly**, so we get better performance. Tables feel **faster and more responsive** compared to views of the same logic.
- ğŸ’¸Â **Improves compute costs.** Compute is significantly more expensive than storage. So while tables use much more storage, itâ€™s generally an economical tradeoff, as you only pay for the transformation compute when you build a table during a job, rather than every time you query it.
- ğŸ”Â **Ideal for models that get queried regularly**, due to the combination of these qualities.
- ğŸ‘Â **Limited to the source data that was available when we did our most recent run.** Weâ€™re â€˜freezingâ€™ the transformation logic into a table. So if we run a model as a table every hour, at 10:59a we still only have data up to 10a, because that was what was available in our source data when we ran the table last at 10a. Only at the next run will the newer data be included in our rebuild.

### Incremental models

- ğŸ§±Â **Incremental** models build a **table** in **pieces over time**, only adding and updating new or changed records.
- ğŸï¸Â  **Builds more quickly** than a regular table of the same logic.
- ğŸ¢Â **Initial runs are slow.** Typically we use incremental models on very large datasets, so building the initial table on the full dataset is time consuming and equivalent to the table materialization.
- ğŸ‘Â **Add complexity.** Incremental models require deeper consideration of layering and timing.
- ğŸ‘Â Can drift from source data over time. As weâ€™re not processing all of the source data when we run an incremental model, extra effort is required to capture changes to historical data.

### Comparing the materialization types

|                      | view                                 | table                                  | incremental                            |
| -------------------- | ------------------------------------ | -------------------------------------- | -------------------------------------- |
| ğŸ› ï¸âŒ›Â **build time**  | ğŸ’šÂ  fastest â€” only stores logic      | â¤ï¸Â  slowest â€” linear to size of data   | ğŸ’›Â  medium â€” builds flexible portion   |
| ğŸ› ï¸ğŸ’¸Â **build costs** | ğŸ’šÂ  lowest â€” no data processed       | â¤ï¸Â  highest â€” all data processed       | ğŸ’›Â  medium â€” some data processed       |
| ğŸ“ŠğŸ’¸Â **query costs** | â¤ï¸Â  higher â€” reprocess every query   | ğŸ’šÂ  lower â€” data in warehouse          | ğŸ’šÂ  lower â€” data in warehouse          |
| ğŸ…ğŸŒ±Â **freshness**   | ğŸ’šÂ Â best â€” up-to-the-minute of query | ğŸ’›Â  moderate â€” up to most recent build | ğŸ’›Â  moderate â€” up to most recent build |
| ğŸ§ ğŸ¤” **complexity**  | ğŸ’š simple - maps to warehouse object | ğŸ’š simple - map to warehouse concept   | ğŸ’› moderate - adds logical complexity  |

:::info
ğŸ”‘ **Time is money.** Notice in the above chart that the time and costs rows contain the same results. This is to highlight that when weâ€™re talking about time in warehouses, weâ€™re talking about compute time, which is the primary driver of costs.
:::
